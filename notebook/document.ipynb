{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356718c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/pdf_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a2c1c3",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a6c728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Practice\\RAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff924af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any, Dict\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0cecbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, directory_path):\n",
    "        self.directory_path = directory_path\n",
    "        # self.all_pdf_document = self.process_all_pdfs()\n",
    "\n",
    "    def process_all_pdfs(self):\n",
    "        all_pdf = []\n",
    "        pdf_dir = Path(self.directory_path)\n",
    "\n",
    "        pdf_files = list(pdf_dir.glob(\"*.pdf\") )\n",
    "    \n",
    "        print(f\"Found {len(pdf_files)} PDF files in {self.directory_path}\")\n",
    "\n",
    "        for pdf_file in pdf_files:\n",
    "            print(f\"Processing file: {pdf_file.name}\")\n",
    "            try:\n",
    "                loader = PyMuPDFLoader(str(pdf_file))\n",
    "                documents = loader.load()\n",
    "\n",
    "                # Add source information to metadata\n",
    "                for doc in documents:\n",
    "                    doc.metadata['source_file'] = pdf_file.name\n",
    "                    doc.metadata['file_type'] = 'pdf'\n",
    "\n",
    "                all_pdf.extend(documents)\n",
    "                print(f\"Loaded {len(documents)} pages from {pdf_file.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {pdf_file.name}: {e}\")\n",
    "\n",
    "        print(f\"Total documents loaded: {len(all_pdf)}\")\n",
    "        return all_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9565b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(\"../data/pdf_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb0139",
   "metadata": {},
   "source": [
    "### Document Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81f50201",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentSplitter:\n",
    "    def __init__(self, documents, chunk_size=1000, chunk_overlap=200):\n",
    "        self.documents = documents\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "        # self.split_documents()\n",
    "\n",
    "    def split_documents(self):\n",
    "        \"\"\"Split documents into smaller chunks.\"\"\"\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap,\n",
    "            length_function=len,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(self.documents)\n",
    "        print(f\"Split {len(self.documents)} documents into {len(chunks)} chunks\")\n",
    "        # Show example of a chunk\n",
    "        if chunks:\n",
    "            print(f\"\\nExample chunk:\")\n",
    "            print(f\"Content: {chunks[0].page_content[:200]}...\")\n",
    "            print(f\"Metadata: {chunks[0].metadata}\")\n",
    "\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "348fa936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 PDF files in ../data/pdf_files\n",
      "Processing file: ALL-IN-ONE RAG FRAMEWORK.pdf\n",
      "Loaded 18 pages from ALL-IN-ONE RAG FRAMEWORK.pdf\n",
      "Processing file: HyperbolicRAG.pdf\n",
      "Loaded 12 pages from HyperbolicRAG.pdf\n",
      "Processing file: RETRIEVAL-AUGMENTED CODE GENERATION.pdf\n",
      "Loaded 38 pages from RETRIEVAL-AUGMENTED CODE GENERATION.pdf\n",
      "Processing file: Retrieval-Augmented Generation with Implicit Queries.pdf\n",
      "Loaded 13 pages from Retrieval-Augmented Generation with Implicit Queries.pdf\n",
      "Processing file: When Retrieval Succeeds and Fails.pdf\n",
      "Loaded 11 pages from When Retrieval Succeeds and Fails.pdf\n",
      "Total documents loaded: 92\n",
      "Split 92 documents into 520 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\n",
      "Zirui Guo, Xubin Ren, Lingrui Xu, Jiahao Zhang, Chao Huang∗\n",
      "The University of Hong Kong\n",
      "zrguo101@hku.hk\n",
      "xubinrencs@gmail.c...\n",
      "Metadata: {'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:e76afa9)', 'creationdate': '', 'source': '..\\\\data\\\\pdf_files\\\\ALL-IN-ONE RAG FRAMEWORK.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\ALL-IN-ONE RAG FRAMEWORK.pdf', 'total_pages': 18, 'format': 'PDF 1.7', 'title': 'RAG-Anything: All-in-One RAG Framework', 'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang', 'subject': '', 'keywords': '', 'moddate': '', 'trapped': '', 'modDate': '', 'creationDate': '', 'page': 0, 'source_file': 'ALL-IN-ONE RAG FRAMEWORK.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "doc_split = DocumentSplitter(data_loader.process_all_pdfs())\n",
    "chunks = doc_split.split_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9457a2",
   "metadata": {},
   "source": [
    "### Embedding and Vector Store DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d4f6360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x22fc547fd40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "141484c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x22fc5759640>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0de3523f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 520 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 17/17 [00:22<00:00,  1.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (520, 384)\n",
      "Adding 520 documents to vector store...\n",
      "Successfully added 520 documents to vector store\n",
      "Total documents in collection: 520\n"
     ]
    }
   ],
   "source": [
    "texts = [chunk.page_content for chunk in chunks]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "vectorstore.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371957fb",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectroStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfd85093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0401240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'Can you tell me about recent publication of RAG?'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 31.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 2 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_6de717ba_466',\n",
       "  'content': 'Gutiérrez et al. [2024, 2025], Leung et al. [2025]. The first line of RAG research focuses on\\ntransforming the original queries to facilitate more effective subsequent retrieval Gao et al. [2023],\\nMa et al. [2023b]; Some other studies aim to fine-tune the embedding model to accurately retrieve\\nthe most relevant content Li and Li [2024], Zhang et al. [2025b]; Recently, much of the research has\\nPreprint.\\narXiv:2510.09106v1  [cs.CL]  10 Oct 2025',\n",
       "  'metadata': {'author': 'Yongjie Wang; Yue Yu; Kaisong Song; Jun Lin; Zhiqi Shen',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:e76afa9)',\n",
       "   'modDate': '',\n",
       "   'source_file': 'When Retrieval Succeeds and Fails.pdf',\n",
       "   'creationDate': '',\n",
       "   'page': 0,\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'keywords': '',\n",
       "   'subject': '',\n",
       "   'title': 'When Retrieval Succeeds and Fails: Rethinking Retrieval-Augmented Generation for LLMs',\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\When Retrieval Succeeds and Fails.pdf',\n",
       "   'creationdate': '',\n",
       "   'file_path': '..\\\\data\\\\pdf_files\\\\When Retrieval Succeeds and Fails.pdf',\n",
       "   'total_pages': 11,\n",
       "   'content_length': 446,\n",
       "   'doc_index': 466,\n",
       "   'moddate': '',\n",
       "   'file_type': 'pdf',\n",
       "   'trapped': '',\n",
       "   'format': 'PDF 1.7'},\n",
       "  'similarity_score': 0.07875305414199829,\n",
       "  'distance': 0.9212469458580017,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_171a8a02_5',\n",
       "  'content': 'RAG-ANYTHING: ALL-IN-ONE RAG FRAMEWORK\\nneed for multimodal RAG capabilities. In Scientific Research, experimental results are primarily\\ncommunicated through plots, diagrams, and statistical visualizations. These contain core discoveries\\nthat remain invisible to text-only systems. Financial Analysis relies heavily on market charts,\\ncorrelation matrices, and performance tables. Investment insights are encoded in visual patterns\\nrather than textual descriptions. Additionally, Medical Literature Analysis depends on radiological\\nimages, diagnostic charts, and clinical data tables. These contain life-critical information essential for\\naccurate diagnosis and treatment decisions. Current RAG frameworks systematically exclude these\\nvital knowledge sources across all three scenarios. This creates fundamental gaps that render them\\ninadequate for real-world applications requiring comprehensive information understanding. Therefore,',\n",
       "  'metadata': {'format': 'PDF 1.7',\n",
       "   'source_file': 'ALL-IN-ONE RAG FRAMEWORK.pdf',\n",
       "   'trapped': '',\n",
       "   'keywords': '',\n",
       "   'doc_index': 5,\n",
       "   'content_length': 932,\n",
       "   'source': '..\\\\data\\\\pdf_files\\\\ALL-IN-ONE RAG FRAMEWORK.pdf',\n",
       "   'total_pages': 18,\n",
       "   'modDate': '',\n",
       "   'subject': '',\n",
       "   'file_path': '..\\\\data\\\\pdf_files\\\\ALL-IN-ONE RAG FRAMEWORK.pdf',\n",
       "   'creator': 'arXiv GenPDF (tex2pdf:e76afa9)',\n",
       "   'creationdate': '',\n",
       "   'creationDate': '',\n",
       "   'producer': 'pikepdf 8.15.1',\n",
       "   'page': 1,\n",
       "   'moddate': '',\n",
       "   'author': 'Zirui Guo; Xubin Ren; Lingrui Xu; Jiahao Zhang; Chao Huang',\n",
       "   'title': 'RAG-Anything: All-in-One RAG Framework',\n",
       "   'file_type': 'pdf'},\n",
       "  'similarity_score': 0.0615154504776001,\n",
       "  'distance': 0.9384845495223999,\n",
       "  'rank': 2}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"Can you tell me about recent publication of RAG?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43657ed6",
   "metadata": {},
   "source": [
    "### VectorDB To LLM Output Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bf3c22e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM\n",
    "\n",
    "# import sys\n",
    "# print(sys.executable)\n",
    "\n",
    "# LLM\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel\n",
    "from google.oauth2 import service_account\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# print(load_dotenv())\n",
    "# print(os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aad8feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiRAG:\n",
    "    \"\"\"Complete RAG pipeline with Gemini LLM via Vertex AI\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        service_account_key_path: str = None,\n",
    "        project_id: str = None,\n",
    "        location: str = \"us-central1\",\n",
    "        model_name: str = \"gemini-2.5-flash\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize Gemini via Vertex AI\n",
    "        \n",
    "        Args:\n",
    "            service_account_key_path: Path to service account JSON key file\n",
    "            project_id: Google Cloud project ID\n",
    "            location: Google Cloud location (default: us-central1)\n",
    "            model_name: Gemini model name (default: gemini-2.5-flash)\n",
    "        \"\"\"\n",
    "        # Get configuration from parameters or environment\n",
    "        self.key_path = service_account_key_path or os.getenv(\"VERTEX_AI_KEY_PATH\")\n",
    "        self.project_id = project_id or os.getenv(\"VERTEX_AI_PROJECT_ID\")\n",
    "        self.location = location or os.getenv(\"VERTEX_AI_LOCATION\", \"us-central1\")\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        if not self.key_path or not self.project_id:\n",
    "            raise ValueError(\n",
    "                \"Service account key path and project ID required. \"\n",
    "                \"Set VERTEX_AI_KEY_PATH and VERTEX_AI_PROJECT_ID in .env or pass as parameters\"\n",
    "            )\n",
    "        \n",
    "        # Initialize Vertex AI with service account\n",
    "        try:\n",
    "            creds = service_account.Credentials.from_service_account_file(self.key_path)\n",
    "            vertexai.init(\n",
    "                project=self.project_id,\n",
    "                location=self.location,\n",
    "                credentials=creds\n",
    "            )\n",
    "            \n",
    "            self.model = GenerativeModel(self.model_name)\n",
    "            print(f\"Vertex AI Gemini initialized: {self.model_name}\")\n",
    "            print(f\"Project: {self.project_id}, Location: {self.location}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing Vertex AI: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def generate_response(self, query: str, retrieved_docs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate response using Gemini with retrieved context\"\"\"\n",
    "        \n",
    "        # Prepare context from retrieved documents\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document {i+1} (Score: {doc['similarity_score']:.3f}):\\n{doc['content']}\"\n",
    "            for i, doc in enumerate(retrieved_docs)\n",
    "        ])\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"\"\"Use the following context to answer the question. If you don't know the answer based on the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        try:\n",
    "            print(\"Generating response with Gemini...\")\n",
    "            # Use native SDK's generate_content method\n",
    "            response = self.model.generate_content(prompt)\n",
    "            \n",
    "            return {\n",
    "                'answer': response.text,\n",
    "                'query': query,\n",
    "                'num_sources': len(retrieved_docs),\n",
    "                'sources': [\n",
    "                    {\n",
    "                        'rank': doc['rank'],\n",
    "                        'score': doc['similarity_score'],\n",
    "                        'source': doc['metadata'].get('source_file', 'Unknown'),\n",
    "                        'page': doc['metadata'].get('page', 'N/A')\n",
    "                    }\n",
    "                    for doc in retrieved_docs\n",
    "                ]\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response: {e}\")\n",
    "            return {\n",
    "                'answer': f\"Error generating response: {str(e)}\",\n",
    "                'query': query,\n",
    "                'num_sources': 0,\n",
    "                'sources': []\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75b51715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Pipeline\n",
    "class RAGPipeline:\n",
    "    \"\"\"Complete RAG pipeline orchestrator\"\"\"\n",
    "    \n",
    "    def __init__(self, pdf_directory: str, persist_directory: str = \"../data/vector_store\"):\n",
    "        self.pdf_directory = pdf_directory\n",
    "        self.persist_directory = persist_directory\n",
    "        \n",
    "        # Initialize components\n",
    "        self.embedding_manager = EmbeddingManager()\n",
    "        self.vector_store = VectorStore(persist_directory=persist_directory)\n",
    "        self.retriever = RAGRetriever(self.vector_store, self.embedding_manager)\n",
    "        self.gemini_rag = GeminiRAG()\n",
    "    \n",
    "    def index_documents(self, chunk_size: int = 1000, chunk_overlap: int = 200):\n",
    "        \"\"\"Load, split, embed, and index documents\"\"\"\n",
    "        print(\"=\"*50)\n",
    "        print(\"Starting document indexing...\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Load documents\n",
    "        data_loader = DataLoader(self.pdf_directory)\n",
    "        documents = data_loader.process_all_pdfs()\n",
    "        \n",
    "        # Split documents\n",
    "        doc_splitter = DocumentSplitter(documents, chunk_size, chunk_overlap)\n",
    "        chunks = doc_splitter.split_documents()\n",
    "        \n",
    "        # Generate embeddings\n",
    "        texts = [chunk.page_content for chunk in chunks]\n",
    "        embeddings = self.embedding_manager.generate_embeddings(texts)\n",
    "        \n",
    "        # Add to vector store\n",
    "        self.vector_store.add_documents(chunks, embeddings)\n",
    "        \n",
    "        print(\"=\"*50)\n",
    "        print(\"Document indexing complete!\")\n",
    "        print(\"=\"*50)\n",
    "    \n",
    "    def query(self, question: str, top_k: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"Query the RAG pipeline\"\"\"\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"Processing query: {question}\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        retrieved_docs = self.retriever.retrieve(question, top_k=top_k)\n",
    "        \n",
    "        if not retrieved_docs:\n",
    "            return {\n",
    "                'answer': \"No relevant documents found to answer your question.\",\n",
    "                'query': question,\n",
    "                'num_sources': 0,\n",
    "                'sources': []\n",
    "            }\n",
    "        \n",
    "        # Generate response\n",
    "        response = self.gemini_rag.generate_response(question, retrieved_docs)\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd3a816e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n",
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 520\n",
      "Vertex AI Gemini initialized: gemini-2.5-flash\n",
      "Project: vertex-ai-learning-480916, Location: us-central1\n"
     ]
    }
   ],
   "source": [
    "pipeline = RAGPipeline(pdf_directory=\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9c9f8e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Processing query: Can you give some modern knowlodge about RAG\n",
      "==================================================\n",
      "Retrieving documents for query: 'Can you give some modern knowlodge about RAG'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 27.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Generating response with Gemini...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "response = pipeline.query(\"Can you give some modern knowlodge about RAG\", top_k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2169dcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: Based on the context, here is some modern knowledge about RAG:\n",
      "\n",
      "Modern RAG frameworks are increasingly focusing on multimodal capabilities. Traditional RAG systems, which are often text-only, systematically exclude vital knowledge sources such as plots, diagrams, statistical visualizations, market charts, correlation matrices, performance tables, radiological images, diagnostic charts, and clinical data tables. This exclusion creates fundamental gaps, making them inadequate for real-world applications that require comprehensive information understanding in fields like scientific research, financial analysis, and medical literature analysis.\n",
      "\n",
      "While current multimodal RAG systems show promising capabilities, they still have limitations and challenges. Researchers are actively investigating failure patterns to understand where and why these systems break down, which is crucial for advancing the field beyond current performance plateaus.\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nAnswer: {response['answer']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
